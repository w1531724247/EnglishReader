    About AVFoundation
    Important: This is a preliminary document for an API or technology in development. Apple is supplying this
information to help you plan for the adoption of the technologies and programming interfaces described herein for
use on Apple-branded products. This information is subject to change, and software implemented according to this
document should be tested with final operating system software and final documentation. Newer versions of this
document may be provided with future betas of the API or technology.

AVFoundation is one of several frameworks that you can use to play and create time-based audiovisual media. It provides an Objective-C interface you use to work on a detailed level with time-based audiovisual data. For example, you can use it to examine, create, edit, or reencode media files. You can also get input streams from devices and manipulate video during realtime capture and playback. Figure I-1 shows the architecture on iOS.
Figure I-1  AVFoundation stack on iOS

Figure I-2 shows the corresponding media architecture on OS X.
Figure I-2  AVFoundation stack on OS X

You should typically use the highest-level abstraction available that allows you to perform the tasks you want.
•	If you simply want to play movies, use the AVKit framework.
•	On iOS, to record video when you need only minimal control over format, use the UIKit framework (UIImagePickerController).
Note, however, that some of the primitive data structures that you use in AV Foundation—including time-related data structures and opaque objects to carry and describe media data—are declared in the Core Media framework.

At a Glance
There are two facets to the AVFoundation framework—APIs related to video and APIs related just to audio. The older audio-related classes provide easy ways to deal with audio. They are described in the Multimedia Programming Guide, not in this document.
•	To play sound files, you can use AVAudioPlayer.
•	To record audio, you can use AVAudioRecorder.
You can also configure the audio behavior of your application using AVAudioSession; this is described in Audio Session Programming Guide.
Representing and Using Media with AVFoundation

The primary class that the AV Foundation framework uses to represent media is AVAsset. The design of the framework is largely guided by this representation. Understanding its structure will help you to understand how the framework works. An AVAsset instance is an aggregated representation of a collection of one or more pieces of media data (audio and video tracks). It provides information about the collection as a whole, such as its title, duration, natural presentation size, and so on. AVAsset is not tied to particular data format. AVAsset is the superclass of other classes used to create asset instances from media at a URL (see Using Assets) and to create new compositions (see Editing).
Each of the individual pieces of media data in the asset is of a uniform type and called a track. In a typical simple case, one track represents the audio component, and another represents the video component; in a complex composition, however, there may be multiple overlapping tracks of audio and video. Assets may also have metadata.
A vital concept in AV Foundation is that initializing an asset or a track does not necessarily mean that it is ready for use. It may require some time to calculate even the duration of an item (an MP3 file, for example, may not contain summary information). Rather than blocking the current thread while a value is being calculated, you ask for values and get an answer back asynchronously through a callback that you define using a block.
Relevant Chapters: Using Assets, Time and Media Representations

